import os
import hashlib
import sqlite3
import threading
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Set
from dataclasses import dataclass
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict
import json

# Configuration
DB_FILE = "cloud_database.db"
HASH_ALGORITHM = "blake2b"  # Fast cryptographic hash
SIMILARITY_THRESHOLD = 0.9  # Threshold for considering data as redundant
MAX_WORKERS = 4  # For parallel processing

@dataclass
class DataEntry:
    id: str
    content: str
    content_hash: str
    source: str
    timestamp: str
    is_verified: bool = False
    is_redundant: bool = False

class DataValidator:
    """Implements validation mechanisms to check data uniqueness"""
    
    def __init__(self):
        self.fingerprints: Dict[str, Set[str]] = defaultdict(set)
        self.lock = threading.Lock()
    
    @staticmethod
    def compute_content_hash(content: str) -> str:
        """Compute content hash using selected algorithm"""
        if HASH_ALGORITHM == "blake2b":
            return hashlib.blake2b(content.encode('utf-8')).hexdigest()
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def compute_similarity(self, content1: str, content2: str) -> float:
        """Compute similarity between two text contents"""
        # Using Jaccard similarity for demonstration
        words1 = set(content1.split())
        words2 = set(content2.split())
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        return intersection / union if union != 0 else 0
    
    def is_redundant(self, new_content: str) -> Tuple[bool, Optional[str]]:
        """Check if content is redundant with existing data"""
        new_hash = self.compute_content_hash(new_content)
        
        # First check exact matches
        with self.lock:
            if new_hash in self.fingerprints['exact']:
                return True, "Exact duplicate"
            
            # Then check similar content
            for existing_content in self.fingerprints['content']:
                similarity = self.compute_similarity(new_content, existing_content)
                if similarity > SIMILARITY_THRESHOLD:
                    return True, f"High similarity ({similarity:.2f})"
        
        return False, None

    def add_content(self, content: str):
        """Add verified content to fingerprints"""
        with self.lock:
            self.fingerprints['exact'].add(self.compute_content_hash(content))
            self.fingerprints['content'].add(content)

class RedundancyChecker:
    """Main class for redundancy detection and management"""
    
    def __init__(self):
        self.validator = DataValidator()
        self.db_conn = self.init_database()
        self.cache = {}  # Cache for frequent content
        self.stats = {
            "total_processed": 0,
            "duplicates_found": 0,
            "false_positives": 0
        }
    
    def init_database(self) -> sqlite3.Connection:
        """Initialize the database connection"""
        db_exists = os.path.exists(DB_FILE)
        conn = sqlite3.connect(DB_FILE)
        
        if not db_exists:
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE data_entries (
                    id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    content_hash TEXT NOT NULL,
                    source TEXT,
                    timestamp TEXT NOT NULL,
                    is_verified INTEGER DEFAULT 0,
                    is_redundant INTEGER DEFAULT 0,
                    UNIQUE(content_hash)
                )
            ''')
            conn.commit()
        
        # Load existing fingerprints
        self.load_existing_fingerprints()
        return conn
    
    def load_existing_fingerprints(self):
        """Load existing content fingerprints from database"""
        cursor = self.db_conn.cursor()
        cursor.execute("SELECT content, content_hash FROM data_entries WHERE is_verified = 1")
        
        with self.validator.lock:
            for row in cursor.fetchall():
                content, content_hash = row
                self.validator.fingerprints['exact'].add(content_hash)
                self.validator.fingerprints['content'].add(content)
        
        print(f"Loaded {len(self.validator.fingerprints['content'])} existing fingerprints")
    
    def generate_id(self, content: str) -> str:
        """Generate unique ID for data entry"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        return f"{timestamp}_{self.validator.compute_content_hash(content)[:8]}"
    
    def verify_data_entry(self, entry: DataEntry) -> bool:
        """Validate a data entry against quality rules"""
        if not entry.content or len(entry.content) < 10:
            return False
        
        # Add any additional validation rules here
        return True
    
    def classify_entry(self, entry: DataEntry) -> Tuple[bool, Optional[str]]:
        """
        Classify an entry as redundant, unique, or false positive
        Returns tuple: (is_redundant, reason)
        """
        # Check in cache first
        cache_key = entry.content_hash
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # Perform validation
        is_redundant, reason = self.validator.is_redundant(entry.content)
        
        # Store result in cache
        self.cache[cache_key] = (is_redundant, reason)
        return is_redundant, reason
    
    def add_data(self, content: str, source: str) -> Tuple[bool, str]:
        """Process new data entry and add to database if unique"""
        self.stats["total_processed"] += 1
        
        content_hash = self.validator.compute_content_hash(content)
        entry_id = self.generate_id(content)
        timestamp = datetime.now().isoformat()
        
        # Create data entry object
        entry = DataEntry(
            id=entry_id,
            content=content,
            content_hash=content_hash,
            source=source,
            timestamp=timestamp
        )
        
        # First verify data quality
        if not self.verify_data_entry(entry):
            return False, "Data validation failed"
        
        # Check for redundancy
        is_redundant, reason = self.classify_entry(entry)
        
        if is_redundant:
            self.stats["duplicates_found"] += 1
            entry.is_redundant = True
            
            # Save as redundant entry (with a different flag)
            self._save_to_database(entry)
            return False, f"Redundant data: {reason}"
        
        # Valid unique data - proceed to save
        entry.is_verified = True
        self._save_to_database(entry)
        
        # Add to fingerprint collection
        self.validator.add_content(entry.content)
        return True, "Data added successfully"
    
    def _save_to_database(self, entry: DataEntry):
        """Save entry to database (thread-safe)"""
        try:
            cursor = self.db_conn.cursor()
            cursor.execute('''
                INSERT OR IGNORE INTO data_entries 
                (id, content, content_hash, source, timestamp, is_verified, is_redundant)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                entry.id,
                entry.content,
                entry.content_hash,
                entry.source,
                entry.timestamp,
                int(entry.is_verified),
                int(entry.is_redundant)
            ))
            self.db_conn.commit()
        except sqlite3.Error as e:
            print(f"Database error: {e}")
            self.db_conn.rollback()
    
    def process_batch(self, data_items: List[Tuple[str, str]]) -> Dict:
        """Process multiple data items in batch"""
        results = {
            "added": 0,
            "duplicates": 0,
            "invalid": 0
        }
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = []
            for content, source in data_items:
                futures.append(executor.submit(self.add_data, content, source))
            
            for future in concurrent.futures.as_completed(futures):
                success, message = future.result()
                if success:
                    results["added"] += 1
                elif "Redundant" in message:
                    results["duplicates"] += 1
                else:
                    results["invalid"] += 1
        
        return results
    
    def get_stats(self) -> Dict:
        """Get processing statistics"""
        return {
            **self.stats,
            "fingerprint_count": len(self.validator.fingerprints['content']),
            "cache_size": len(self.cache)
        }

class CloudDatabaseAPI:
    """Simulates cloud database interface"""
    
    def __init__(self):
        self.checker = RedundancyChecker()
    
    def upload_data(self, content: str, source: str) -> Dict:
        """API endpoint for uploading data"""
        success, message = self.checker.add_data(content, source)
        return {
            "status": "success" if success else "error",
            "message": message
        }
    
    def upload_batch(self, data_items: List[Dict]) -> Dict:
        """API endpoint for batch upload"""
        items = [(item['content'], item['source']) for item in data_items]
        results = self.checker.process_batch(items)
        
        return {
            "status": "success",
            "results": results,
            "stats": self.checker.get_stats()
        }
    
    def get_statistics(self) -> Dict:
        """API endpoint for getting statistics"""
        return {
            "status": "success",
            "stats": self.checker.get_stats()
        }

# Example Usage
if __name__ == "__main__":
    api = CloudDatabaseAPI()
    
    # Example single upload
    print("Uploading single item:")
    result = api.upload_data("This is some unique content to store in the database", "source1")
    print(json.dumps(result, indent=2))
    
    # Example batch upload
    print("\nUploading batch:")
    batch = [
        {"content": "This is duplicate content", "source": "source1"},
        {"content": "This is some unique content", "source": "source2"},
        {"content": "This is duplicate content", "source": "source3"},
        {"content": "Too short", "source": "source4"}
    ]
    result = api.upload_batch(batch)
    print(json.dumps(result, indent=2))
    
    # Get statistics
    print("\nCurrent statistics:")
    print(json.dumps(api.get_statistics(), indent=2))
